{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How it works</h3>\n",
    "\n",
    "1. Clean the dataset\n",
    "2. Divide it into train and test sets\n",
    "3. Train the machine using models (RF (priority), KNN)\n",
    "4. Using a script, turns a url into data (like the ones in the csv)\n",
    "5. Put it into the machine to make a prediction whether it's malicious or benign\n",
    "\n",
    "<h3>Main Features</h3>\n",
    "\n",
    "1. path_token_count\n",
    "2. avgpathtokenlen\n",
    "3. symbolcount_url\n",
    "4. symbolcount_filename\n",
    "5. tld\n",
    "6. url_length\n",
    "\n",
    "#locate to visual.ipynb for heatmap and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import urllib.parse\n",
    "import math\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\arell\\Documents\\1_ALF\\data\\All.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Script to turn URL to data</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Helper function to compute entropy (measures randomness in the string)\n",
    "def calculate_entropy(string):\n",
    "    probability = [float(string.count(c)) / len(string) for c in set(string)]\n",
    "    entropy = - sum([p * math.log2(p) for p in probability])\n",
    "    return entropy\n",
    "\n",
    "# Function to tokenize the path\n",
    "def tokenize_path(path):\n",
    "    return re.split(r'[\\/\\-\\._]', path)  # Split based on common delimiters\n",
    "\n",
    "# Function to calculate average token length\n",
    "def avg_token_len(tokens):\n",
    "    return sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "\n",
    "# Function to calculate path token count\n",
    "def path_token_count(path):\n",
    "    tokens = tokenize_path(path)\n",
    "    return len(tokens)\n",
    "\n",
    "# Function to calculate average path token length\n",
    "def avg_path_token_len(path):\n",
    "    tokens = tokenize_path(path)\n",
    "    return avg_token_len(tokens)\n",
    "\n",
    "# Function to count symbols in a string (for symbolcount_url and symbolcount_filename)\n",
    "def count_symbols(string):\n",
    "    return len(re.findall(r'[!@#$%^&*()_+=\\[\\]{};:\"\\\\|,.<>/?]', string))\n",
    "\n",
    "# Function to extract the TLD (top-level domain)\n",
    "def get_tld(domain):\n",
    "    return domain.split('.')[-1]\n",
    "\n",
    "# Function to extract filename from the path\n",
    "def extract_filename(path):\n",
    "    if '/' in path:\n",
    "        return path.split('/')[-1]\n",
    "    return ''\n",
    "\n",
    "# Function to extract various URL features\n",
    "def extract_url_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Extract path, domain, and filename\n",
    "    path = parsed_url.path\n",
    "    domain = parsed_url.netloc\n",
    "    filename = extract_filename(path)\n",
    "    \n",
    "    # Feature calculations\n",
    "    features = {}\n",
    "    features['path_token_count'] = path_token_count(path)\n",
    "    features['avgpathtokenlen'] = avg_path_token_len(path)\n",
    "    \n",
    "    # Count symbols in the URL and filename\n",
    "    features['symbolcount_url'] = count_symbols(url)\n",
    "    features['symbolcount_filename'] = count_symbols(filename)\n",
    "    \n",
    "    # Extract top-level domain (TLD)\n",
    "    features['tld'] = get_tld(domain)\n",
    "    \n",
    "    # URL length\n",
    "    features['url_length'] = len(url)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"http://example.com/path/to/resource.html?query=test\"\n",
    "url_features = extract_url_features(url)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_url_features = pd.DataFrame([url_features])\n",
    "\n",
    "# Display the extracted features\n",
    "print(df_url_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Insert Model!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
