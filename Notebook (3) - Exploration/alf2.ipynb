{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\arell\\Documents\\1_ALF\\data\\All.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features based on PCA:\n",
      "SymbolCount_Afterpath        6.372416\n",
      "argPathRatio                 6.336992\n",
      "SymbolCount_Directoryname    6.295303\n",
      "NumberofDotsinURL            6.207607\n",
      "Directory_LetterCount        6.084013\n",
      "path_token_count             6.081245\n",
      "NumberRate_URL               6.077971\n",
      "Entropy_Afterpath            6.033454\n",
      "SymbolCount_URL              6.033001\n",
      "avgpathtokenlen              6.016338\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Remove rows with missing values (NaN)\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Step 2: Separate features and labels\n",
    "# Assuming the target labels are in a column 'URL_Type_obf_Type', update as necessary\n",
    "X = df_cleaned.drop(columns=['URL_Type_obf_Type'])  # Features\n",
    "y = df_cleaned['URL_Type_obf_Type']  # Target variable\n",
    "\n",
    "# Step 3: Standardize the feature matrix\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Apply PCA to find the top features\n",
    "pca = PCA(n_components=X.shape[1])  # Keep all components initially\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Get the explained variance ratio for each component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Get the loadings for each feature (how much each feature contributes to the principal components)\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i}' for i in range(1, X.shape[1] + 1)], index=X.columns)\n",
    "\n",
    "# Step 5: Find the top 10 features based on their contribution to the principal components\n",
    "# Summing the absolute values of the loadings across all principal components\n",
    "top_features = loadings.abs().sum(axis=1).sort_values(ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 features\n",
    "print(\"Top 10 features based on PCA:\")\n",
    "print(top_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
